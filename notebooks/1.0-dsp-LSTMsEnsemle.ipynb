{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from data.dataset import loadingDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create results folder\n",
    "!mkdir -p ../models/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /data/anaconda/envs/py35/lib/python3.5/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (3.0.4)\n",
      "Requirement already satisfied: tqdm in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (4.28.1)\n",
      "Requirement already satisfied: requests in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (2.18.4)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (1.11.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (1.22)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (2018.8.24)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (2.6)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nkAwjp1TRB-wnOYBvlRJS_srv2c6Spz7\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/opp.mat\n",
      "177MB [00:00, 190MB/s]  \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KJ04DWE7nt_PB0Zm9ZaN-Wh-ZYgvBOj-\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/pamap2.mat\n",
      "140MB [00:00, 150MB/s]  \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15Q8oV02h2_e94IWJ9rnKLrSCKPCTW5FS\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/skoda.mat\n",
      "114MB [00:01, 112MB/s]  \n"
     ]
    }
   ],
   "source": [
    "# run below commands to download datasets from google drive using Gdown tool\n",
    "# Alternatively you can manually download datasets from following url and put them in the data folder\n",
    "# https://goo.gl/wgEuhu\n",
    "\n",
    "!pip install gdown\n",
    "!mkdir -p ../data/processed\n",
    "!gdown https://drive.google.com/uc?id=1nkAwjp1TRB-wnOYBvlRJS_srv2c6Spz7 -O ../data/processed/opp.mat\n",
    "!gdown https://drive.google.com/uc?id=1KJ04DWE7nt_PB0Zm9ZaN-Wh-ZYgvBOj- -O ../data/processed/pamap2.mat\n",
    "!gdown https://drive.google.com/uc?id=15Q8oV02h2_e94IWJ9rnKLrSCKPCTW5FS -O ../data/processed/skoda.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/opp.mat\n",
      "normalising... zero mean, unit variance\n",
      "normalising...X_train, X_valid, X_test... done\n",
      "loading the 79-dim matData successfully . . .\n",
      "\n",
      "Train data shape: inputs(650972, 79), targets (650972,)\n",
      "Valid data shape: inputs(32224, 79), targets (32224,)\n",
      "Test data shape: inputs(118750, 79), targets (118750,)\n"
     ]
    }
   ],
   "source": [
    "#1 is Opportunity , 2 is PAMAP2, 3 is Skoda\n",
    "dataset = 1\n",
    "\n",
    "if dataset == 1:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 79)\n",
    "\tn_classes = 18\n",
    "\tDB = 79\n",
    "if dataset == 2:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 52)\n",
    "\tn_classes = 12\n",
    "\tDB = 52\n",
    "if dataset == 3:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 60)\n",
    "\tn_classes = 11\n",
    "\tDB = 60\n",
    "    \n",
    "print(\"\\nTrain data shape: inputs{0}, targets {1}\".format(train_x.shape, train_y.shape))\n",
    "print(\"Valid data shape: inputs{0}, targets {1}\".format(valid_x.shape, valid_y.shape))\n",
    "print(\"Test data shape: inputs{0}, targets {1}\".format(test_x.shape ,test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid data shape: inputs(1, 32224, 79), targets (1, 32224)\n",
      "Test data shape: inputs(1, 118750, 79), targets (1, 118750)\n"
     ]
    }
   ],
   "source": [
    "DIM = len(train_x[0])\n",
    "TEST_WIN = 5000\n",
    "\n",
    "valid_bt = 1\n",
    "valid_se = len(valid_x)//valid_bt\n",
    "valid_x = valid_x[:valid_se*valid_bt,]\n",
    "valid_y = np.array(valid_y)\n",
    "valid_y = valid_y[:valid_se*valid_bt,]\n",
    "valid_x = np.reshape(valid_x, (valid_bt, -1, DB))\n",
    "valid_y = np.reshape(valid_y, (valid_bt,-1))\n",
    "print(\"Valid data shape: inputs{0}, targets {1}\".format(valid_x.shape, valid_y.shape))\n",
    "\n",
    "test_bt = 1\n",
    "test_se = len(test_x)//test_bt\n",
    "test_x = test_x[:test_se*test_bt,]\n",
    "test_y = np.array(test_y)\n",
    "test_y = test_y[:test_se*test_bt,]\n",
    "test_x = np.reshape(test_x, (test_bt, -1, DB))\n",
    "test_y = np.reshape(test_y, (test_bt,-1))\n",
    "print(\"Test data shape: inputs{0}, targets {1}\".format(test_x.shape ,test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_training_set(train_x, train_y, batch_size):\n",
    "    \n",
    "    seqence_len = len(train_x)//batch_size\n",
    "    \n",
    "    # generate random initial position of sampling for each epoch\n",
    "    indices_start = np.random.randint(low=0, high=len(train_x)-seqence_len, size=(batch_size,))\n",
    "    \n",
    "    indices_all_2d = np.zeros((batch_size, seqence_len))\n",
    "    for i in range(batch_size):\n",
    "        indices_all_2d[i,:] = np.arange(indices_start[i],indices_start[i]+seqence_len)\n",
    "    indices_all = np.reshape(indices_all_2d, (-1))\n",
    "\n",
    "    X_train = np.zeros((batch_size, seqence_len, DIM), dtype=np.float32)\n",
    "    y_train = np.zeros((batch_size, seqence_len), dtype=np.uint8) \n",
    "    for i in range(batch_size):\n",
    "        idx_start = indices_start[i]\n",
    "        idx_end = idx_start+seqence_len\n",
    "        X_train[i,:,:] = train_x[idx_start:idx_end, :]\n",
    "        y_train[i,:] = train_y[idx_start:idx_end]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels=DB, n_hidden=256, n_layers=2, \n",
    "                 n_classes=n_classes, drop_prob=0.5):\n",
    "        super(SingleModel, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        self.lstm  = nn.LSTM(n_channels, n_hidden, n_layers, dropout=self.drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)    \n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden\n",
    "    \n",
    "net = SingleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleModel(\n",
       "  (lstm): LSTM(79, 256, num_layers=2, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "net.apply(init_weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(criterion):\n",
    "    \n",
    "    val_accuracy=0\n",
    "    val_f1score=0\n",
    "    val_losses = []\n",
    "    num_val_process = valid_se//TEST_WIN + 1\n",
    "    val_h = net.init_hidden(valid_bt)\n",
    "    net.eval()\n",
    "\n",
    "    for j in range(num_val_process):\n",
    "        start = j*TEST_WIN\n",
    "        end = np.min((valid_se, start+TEST_WIN))\n",
    "        \n",
    "        x = valid_x[:,start:end,:]\n",
    "        y = valid_y[:,start:end]\n",
    "\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        \n",
    "        output, val_h = net(inputs, val_h, valid_bt)\n",
    "\n",
    "        val_loss = criterion(output, targets.long())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == targets.view(*top_class.shape).long()\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        val_f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='macro')\n",
    "            \n",
    "    test_accuracy=0\n",
    "    test_f1score=0\n",
    "    test_losses = []\n",
    "    num_test_process = test_se//TEST_WIN + 1\n",
    "    test_h = net.init_hidden(test_bt)\n",
    "    \n",
    "    for j in range(num_test_process):\n",
    "        start = j*TEST_WIN\n",
    "        end = np.min((test_se, start+TEST_WIN))\n",
    "        \n",
    "        x = test_x[:,start:end,:]\n",
    "        y = test_y[:,start:end]\n",
    "\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        test_h = tuple([each.data for each in test_h])\n",
    "        \n",
    "        output, test_h = net(inputs, test_h, test_bt)\n",
    "\n",
    "        test_loss = criterion(output, targets.long())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == targets.view(*top_class.shape).long()\n",
    "        test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        test_f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='macro')\n",
    "        \n",
    "    valid_losses_avg = np.mean(val_losses)\n",
    "    valid_f1_avg = val_f1score/num_val_process\n",
    "    print(' '*16 +\"Val   Loss: {:.4f}...\".format(valid_losses_avg),\n",
    "    \"Val  Acc: {:.4f}...\".format(val_accuracy/num_val_process),\n",
    "    \"Val  F1: {:.4f}...\".format(valid_f1_avg))\n",
    "          \n",
    "    test_losses_avg = np.mean(test_losses)\n",
    "    test_f1_avg = test_f1score/num_test_process\n",
    "    print(' '*16 +\"Test  Loss: {:.4f}...\".format(test_losses_avg),\n",
    "    \"Test Acc: {:.4f}...\".format(test_accuracy/num_test_process),\n",
    "    \"Test F1: {:.4f}...\".format(test_f1_avg))\n",
    "    \n",
    "    net.train() # reset to train mode after iterationg through validation data\n",
    "    \n",
    "    return valid_losses_avg, test_losses_avg, valid_f1_avg, test_f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=100, lr=0.001):\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "     \n",
    "    train_losses = []    \n",
    "    results = np.empty([0, 5], dtype=np.float32)\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        train_loss = 0\n",
    "        train_sz = 0\n",
    "        \n",
    "        #generate random batch size for each epoch\n",
    "        batch_size = np.random.randint(low=128, high=256, size=1)[0]\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)      \n",
    "        \n",
    "        x_train, y_train = making_training_set(train_x, train_y, batch_size)\n",
    "        train_len = len(train_x)//batch_size\n",
    "\n",
    "        pos_start = 0\n",
    "        pos_end = 0\n",
    "        while pos_end < train_len:\n",
    "\n",
    "            # generate a random window length in each training process\n",
    "            curr_win_len = np.random.randint(low=16, high=32, size=1)[0]\n",
    "            \n",
    "            pos_start = pos_end\n",
    "            pos_end += curr_win_len\n",
    "\n",
    "            x = x_train[:,pos_start:pos_end,:]\n",
    "            y = y_train[:,pos_start:pos_end]\n",
    "                        \n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            \n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            opt.zero_grad()   \n",
    "            \n",
    "            output, h = net(inputs, h, batch_size)\n",
    "            \n",
    "            loss = criterion(output, targets.long())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            sample_sz = batch_size*curr_win_len\n",
    "            train_loss += loss.item()*sample_sz\n",
    "            train_sz += sample_sz\n",
    "                      \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        #saving the models\n",
    "        PATH = '../models/'+str(DB)+'_'+str(epoch)+'.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        \n",
    "        train_loss_avg = train_loss/train_sz\n",
    "        print(\"Epoch: {}/{}..\".format(epoch+1, epochs),\n",
    "        \"Train Loss: {:.4f}\".format(train_loss_avg))\n",
    "        \n",
    "        valid_loss, test_loss, valid_f1, test_f1 = validation(criterion)\n",
    "        \n",
    "        #saving the results\n",
    "        epoch_results = np.zeros(5)\n",
    "        \n",
    "        epoch_results[0] = train_loss_avg\n",
    "        epoch_results[1] = valid_loss\n",
    "        epoch_results[2] = test_loss\n",
    "        epoch_results[3] = valid_f1\n",
    "        epoch_results[4] = test_f1\n",
    "        \n",
    "        results = np.float32(np.vstack((results, epoch_results)))\n",
    "        \n",
    "        PATH = '../models/results/'+str(DB)+'.npy'\n",
    "        np.save(PATH, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmEnsemble(n_bestM=20):\n",
    "\n",
    "    PATH = '../models/results/'+str(DB)+'.npy'\n",
    "    results = np.load(PATH)\n",
    "\n",
    "    valid_col = 3 #third column of results is validation f1 \n",
    "    idx_set = np.argsort(results[:,valid_col])[::-1] # sort results based on validation f1\n",
    "\n",
    "    best_models = []\n",
    "    best_models.append(idx_set[:n_bestM]) # store the epoch number of top n models\n",
    "\n",
    "    prob_M = np.zeros((n_bestM, test_y.size, n_classes))\n",
    "    \n",
    "    for i in range(n_bestM):\n",
    "        idx = best_models[0][i]\n",
    "\n",
    "        model = '../models/'+str(DB)+'_'+str(idx)+\".pth\"\n",
    "        net.load_state_dict(torch.load(model))\n",
    "       \n",
    "        if(train_on_gpu):\n",
    "            net.cuda()\n",
    "\n",
    "        num_test_process = test_se//TEST_WIN + 1\n",
    "        test_accuracy=0\n",
    "        test_f1score=0\n",
    "        test_losses = []\n",
    "        test_h = net.init_hidden(test_bt)\n",
    "        prob_2d = np.zeros((test_y.size, n_classes))\n",
    "\n",
    "        net.eval()\n",
    "        for j in range(num_test_process):\n",
    "            start = j*TEST_WIN\n",
    "            end = np.min((test_se, start+TEST_WIN))\n",
    "\n",
    "            x = test_x[:,start:end,:]\n",
    "            y = test_y[:,start:end]\n",
    "\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            test_h = tuple([each.data for each in test_h])\n",
    "            output, test_h = net(inputs, test_h, test_bt)\n",
    "\n",
    "            prob_2d[start*test_bt:end*test_bt,:] = F.softmax(output).cpu().detach().numpy()\n",
    "\n",
    "        prob_M[i,:,:] = prob_2d #store predictions of each of the top n models\n",
    "\n",
    "    prob_avg = np.mean(prob_M[:,:,:], axis=0) #model fusion by calculating the average of probabilities \n",
    "    fused_pred = np.argmax(prob_avg, axis=1)\n",
    "\n",
    "    f1_fused = metrics.f1_score(test_y.flatten(\"F\"), fused_pred, average='macro')\n",
    "\n",
    "    print(\"Ensemble of LSTMs F1-score: {:.4f}\".format(f1_fused))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Train Loss: 0.8390\n",
      "                Val   Loss: 0.5269... Val  Acc: 0.7889... Val  F1: 0.2910...\n",
      "                Test  Loss: 0.5022... Test Acc: 0.8366... Test F1: 0.3334...\n",
      "Epoch: 2/100.. Train Loss: 0.3975\n",
      "                Val   Loss: 0.4060... Val  Acc: 0.8831... Val  F1: 0.3982...\n",
      "                Test  Loss: 0.3886... Test Acc: 0.8821... Test F1: 0.4574...\n",
      "Epoch: 3/100.. Train Loss: 0.3275\n",
      "                Val   Loss: 0.3904... Val  Acc: 0.8763... Val  F1: 0.5397...\n",
      "                Test  Loss: 0.4724... Test Acc: 0.8486... Test F1: 0.4925...\n",
      "Epoch: 4/100.. Train Loss: 0.2937\n",
      "                Val   Loss: 0.3901... Val  Acc: 0.8859... Val  F1: 0.4639...\n",
      "                Test  Loss: 0.3738... Test Acc: 0.8935... Test F1: 0.5081...\n",
      "Epoch: 5/100.. Train Loss: 0.2654\n",
      "                Val   Loss: 0.3726... Val  Acc: 0.8961... Val  F1: 0.5175...\n",
      "                Test  Loss: 0.3596... Test Acc: 0.8992... Test F1: 0.5204...\n",
      "Epoch: 6/100.. Train Loss: 0.2141\n",
      "                Val   Loss: 0.3341... Val  Acc: 0.9085... Val  F1: 0.5725...\n",
      "                Test  Loss: 0.3297... Test Acc: 0.8992... Test F1: 0.5766...\n",
      "Epoch: 7/100.. Train Loss: 0.1773\n",
      "                Val   Loss: 0.3854... Val  Acc: 0.8927... Val  F1: 0.5645...\n",
      "                Test  Loss: 0.3293... Test Acc: 0.9085... Test F1: 0.5623...\n",
      "Epoch: 8/100.. Train Loss: 0.1997\n",
      "                Val   Loss: 0.3879... Val  Acc: 0.8916... Val  F1: 0.4668...\n",
      "                Test  Loss: 0.3569... Test Acc: 0.8991... Test F1: 0.5703...\n",
      "Epoch: 9/100.. Train Loss: 0.1566\n",
      "                Val   Loss: 0.3497... Val  Acc: 0.9099... Val  F1: 0.5769...\n",
      "                Test  Loss: 0.2785... Test Acc: 0.9149... Test F1: 0.5980...\n",
      "Epoch: 10/100.. Train Loss: 0.1116\n",
      "                Val   Loss: 0.4061... Val  Acc: 0.8944... Val  F1: 0.5135...\n",
      "                Test  Loss: 0.2971... Test Acc: 0.9194... Test F1: 0.6598...\n",
      "Epoch: 11/100.. Train Loss: 0.1178\n",
      "                Val   Loss: 0.4196... Val  Acc: 0.8997... Val  F1: 0.5870...\n",
      "                Test  Loss: 0.3383... Test Acc: 0.9176... Test F1: 0.6476...\n",
      "Epoch: 12/100.. Train Loss: 0.1146\n",
      "                Val   Loss: 0.4196... Val  Acc: 0.8948... Val  F1: 0.5762...\n",
      "                Test  Loss: 0.3908... Test Acc: 0.9121... Test F1: 0.6465...\n",
      "Epoch: 13/100.. Train Loss: 0.1211\n",
      "                Val   Loss: 0.4533... Val  Acc: 0.8744... Val  F1: 0.4952...\n",
      "                Test  Loss: 0.3871... Test Acc: 0.9063... Test F1: 0.6162...\n",
      "Epoch: 14/100.. Train Loss: 0.1021\n",
      "                Val   Loss: 0.3637... Val  Acc: 0.9124... Val  F1: 0.6287...\n",
      "                Test  Loss: 0.3410... Test Acc: 0.9120... Test F1: 0.6372...\n",
      "Epoch: 15/100.. Train Loss: 0.0714\n",
      "                Val   Loss: 0.4234... Val  Acc: 0.9056... Val  F1: 0.4874...\n",
      "                Test  Loss: 0.3762... Test Acc: 0.9126... Test F1: 0.6396...\n",
      "Epoch: 16/100.. Train Loss: 0.0869\n",
      "                Val   Loss: 0.4034... Val  Acc: 0.9141... Val  F1: 0.6177...\n",
      "                Test  Loss: 0.3404... Test Acc: 0.9200... Test F1: 0.6680...\n",
      "Epoch: 17/100.. Train Loss: 0.0601\n",
      "                Val   Loss: 0.4001... Val  Acc: 0.9155... Val  F1: 0.5960...\n",
      "                Test  Loss: 0.4121... Test Acc: 0.9118... Test F1: 0.6170...\n",
      "Epoch: 18/100.. Train Loss: 0.0743\n",
      "                Val   Loss: 0.4589... Val  Acc: 0.9080... Val  F1: 0.5915...\n",
      "                Test  Loss: 0.4085... Test Acc: 0.9122... Test F1: 0.6323...\n",
      "Epoch: 19/100.. Train Loss: 0.0845\n",
      "                Val   Loss: 0.4628... Val  Acc: 0.9039... Val  F1: 0.5092...\n",
      "                Test  Loss: 0.4682... Test Acc: 0.8632... Test F1: 0.6095...\n",
      "Epoch: 20/100.. Train Loss: 0.0690\n",
      "                Val   Loss: 0.5738... Val  Acc: 0.8736... Val  F1: 0.5007...\n",
      "                Test  Loss: 0.4264... Test Acc: 0.9129... Test F1: 0.6186...\n",
      "Epoch: 21/100.. Train Loss: 0.0654\n",
      "                Val   Loss: 0.4631... Val  Acc: 0.9069... Val  F1: 0.5346...\n",
      "                Test  Loss: 0.4182... Test Acc: 0.9137... Test F1: 0.6277...\n",
      "Epoch: 22/100.. Train Loss: 0.0714\n",
      "                Val   Loss: 0.4740... Val  Acc: 0.8898... Val  F1: 0.4688...\n",
      "                Test  Loss: 0.4659... Test Acc: 0.9065... Test F1: 0.5725...\n",
      "Epoch: 23/100.. Train Loss: 0.0682\n",
      "                Val   Loss: 0.4795... Val  Acc: 0.9147... Val  F1: 0.6209...\n",
      "                Test  Loss: 0.4230... Test Acc: 0.9152... Test F1: 0.6572...\n",
      "Epoch: 24/100.. Train Loss: 0.0494\n",
      "                Val   Loss: 0.4987... Val  Acc: 0.9062... Val  F1: 0.5486...\n",
      "                Test  Loss: 0.4684... Test Acc: 0.8978... Test F1: 0.6448...\n",
      "Epoch: 25/100.. Train Loss: 0.0543\n",
      "                Val   Loss: 0.4724... Val  Acc: 0.9095... Val  F1: 0.5928...\n",
      "                Test  Loss: 0.4263... Test Acc: 0.9111... Test F1: 0.6180...\n",
      "Epoch: 26/100.. Train Loss: 0.0517\n",
      "                Val   Loss: 0.4634... Val  Acc: 0.9103... Val  F1: 0.6031...\n",
      "                Test  Loss: 0.4635... Test Acc: 0.9095... Test F1: 0.6693...\n",
      "Epoch: 27/100.. Train Loss: 0.0491\n",
      "                Val   Loss: 0.4650... Val  Acc: 0.9135... Val  F1: 0.5801...\n",
      "                Test  Loss: 0.4707... Test Acc: 0.9161... Test F1: 0.6549...\n",
      "Epoch: 28/100.. Train Loss: 0.0678\n",
      "                Val   Loss: 0.4900... Val  Acc: 0.9044... Val  F1: 0.5311...\n",
      "                Test  Loss: 0.4726... Test Acc: 0.9089... Test F1: 0.6212...\n",
      "Epoch: 29/100.. Train Loss: 0.0458\n",
      "                Val   Loss: 0.5327... Val  Acc: 0.8998... Val  F1: 0.5169...\n",
      "                Test  Loss: 0.4677... Test Acc: 0.9179... Test F1: 0.6499...\n",
      "Epoch: 30/100.. Train Loss: 0.0350\n",
      "                Val   Loss: 0.5465... Val  Acc: 0.9048... Val  F1: 0.5239...\n",
      "                Test  Loss: 0.4608... Test Acc: 0.9161... Test F1: 0.6590...\n",
      "Epoch: 31/100.. Train Loss: 0.0398\n",
      "                Val   Loss: 0.5090... Val  Acc: 0.9052... Val  F1: 0.6064...\n",
      "                Test  Loss: 0.4757... Test Acc: 0.9139... Test F1: 0.6580...\n",
      "Epoch: 32/100.. Train Loss: 0.0394\n",
      "                Val   Loss: 0.5186... Val  Acc: 0.9041... Val  F1: 0.5306...\n",
      "                Test  Loss: 0.4998... Test Acc: 0.9129... Test F1: 0.6201...\n",
      "Epoch: 33/100.. Train Loss: 0.0470\n",
      "                Val   Loss: 0.4870... Val  Acc: 0.8990... Val  F1: 0.5024...\n",
      "                Test  Loss: 0.4713... Test Acc: 0.9117... Test F1: 0.6100...\n",
      "Epoch: 34/100.. Train Loss: 0.0570\n",
      "                Val   Loss: 0.4668... Val  Acc: 0.8989... Val  F1: 0.4710...\n",
      "                Test  Loss: 0.4454... Test Acc: 0.9122... Test F1: 0.5905...\n",
      "Epoch: 35/100.. Train Loss: 0.0463\n",
      "                Val   Loss: 0.5543... Val  Acc: 0.9007... Val  F1: 0.4955...\n",
      "                Test  Loss: 0.5177... Test Acc: 0.9077... Test F1: 0.6303...\n",
      "Epoch: 36/100.. Train Loss: 0.0434\n",
      "                Val   Loss: 0.5069... Val  Acc: 0.9031... Val  F1: 0.5328...\n",
      "                Test  Loss: 0.4724... Test Acc: 0.9125... Test F1: 0.6313...\n",
      "Epoch: 37/100.. Train Loss: 0.0278\n",
      "                Val   Loss: 0.5623... Val  Acc: 0.9062... Val  F1: 0.5366...\n",
      "                Test  Loss: 0.5464... Test Acc: 0.9087... Test F1: 0.6618...\n",
      "Epoch: 38/100.. Train Loss: 0.0262\n",
      "                Val   Loss: 0.5621... Val  Acc: 0.9053... Val  F1: 0.5308...\n",
      "                Test  Loss: 0.5563... Test Acc: 0.9151... Test F1: 0.6346...\n",
      "Epoch: 39/100.. Train Loss: 0.0374\n",
      "                Val   Loss: 0.5276... Val  Acc: 0.9084... Val  F1: 0.5375...\n",
      "                Test  Loss: 0.5145... Test Acc: 0.9162... Test F1: 0.5897...\n",
      "Epoch: 40/100.. Train Loss: 0.0264\n",
      "                Val   Loss: 0.5708... Val  Acc: 0.9017... Val  F1: 0.5029...\n",
      "                Test  Loss: 0.5479... Test Acc: 0.9139... Test F1: 0.5939...\n",
      "Epoch: 41/100.. Train Loss: 0.0358\n",
      "                Val   Loss: 0.6070... Val  Acc: 0.8899... Val  F1: 0.5066...\n",
      "                Test  Loss: 0.4744... Test Acc: 0.9121... Test F1: 0.6193...\n",
      "Epoch: 42/100.. Train Loss: 0.0377\n",
      "                Val   Loss: 0.5602... Val  Acc: 0.8958... Val  F1: 0.5013...\n",
      "                Test  Loss: 0.5302... Test Acc: 0.9166... Test F1: 0.6272...\n",
      "Epoch: 43/100.. Train Loss: 0.0241\n",
      "                Val   Loss: 0.6046... Val  Acc: 0.8977... Val  F1: 0.5253...\n",
      "                Test  Loss: 0.5265... Test Acc: 0.9156... Test F1: 0.6296...\n",
      "Epoch: 44/100.. Train Loss: 0.0319\n",
      "                Val   Loss: 0.5526... Val  Acc: 0.9027... Val  F1: 0.5382...\n",
      "                Test  Loss: 0.5234... Test Acc: 0.9104... Test F1: 0.6222...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/100.. Train Loss: 0.0228\n",
      "                Val   Loss: 0.6316... Val  Acc: 0.9009... Val  F1: 0.5316...\n",
      "                Test  Loss: 0.5612... Test Acc: 0.9161... Test F1: 0.6715...\n",
      "Epoch: 46/100.. Train Loss: 0.0365\n",
      "                Val   Loss: 0.5283... Val  Acc: 0.9066... Val  F1: 0.5358...\n",
      "                Test  Loss: 0.5178... Test Acc: 0.9082... Test F1: 0.6017...\n",
      "Epoch: 47/100.. Train Loss: 0.0297\n",
      "                Val   Loss: 0.5853... Val  Acc: 0.9014... Val  F1: 0.5289...\n",
      "                Test  Loss: 0.5424... Test Acc: 0.9100... Test F1: 0.6148...\n",
      "Epoch: 48/100.. Train Loss: 0.0317\n",
      "                Val   Loss: 0.5766... Val  Acc: 0.9005... Val  F1: 0.5314...\n",
      "                Test  Loss: 0.4976... Test Acc: 0.9113... Test F1: 0.6497...\n",
      "Epoch: 49/100.. Train Loss: 0.0333\n",
      "                Val   Loss: 0.5773... Val  Acc: 0.8998... Val  F1: 0.5303...\n",
      "                Test  Loss: 0.5750... Test Acc: 0.9102... Test F1: 0.6101...\n",
      "Epoch: 50/100.. Train Loss: 0.0261\n",
      "                Val   Loss: 0.5577... Val  Acc: 0.9025... Val  F1: 0.5327...\n",
      "                Test  Loss: 0.5128... Test Acc: 0.9161... Test F1: 0.6289...\n",
      "Epoch: 51/100.. Train Loss: 0.0197\n",
      "                Val   Loss: 0.5386... Val  Acc: 0.9079... Val  F1: 0.5342...\n",
      "                Test  Loss: 0.5320... Test Acc: 0.9216... Test F1: 0.6352...\n",
      "Epoch: 52/100.. Train Loss: 0.0253\n",
      "                Val   Loss: 0.5908... Val  Acc: 0.9064... Val  F1: 0.5887...\n",
      "                Test  Loss: 0.5309... Test Acc: 0.9165... Test F1: 0.6646...\n",
      "Epoch: 53/100.. Train Loss: 0.0260\n",
      "                Val   Loss: 0.5995... Val  Acc: 0.9041... Val  F1: 0.5187...\n",
      "                Test  Loss: 0.5134... Test Acc: 0.9159... Test F1: 0.6210...\n",
      "Epoch: 54/100.. Train Loss: 0.0215\n",
      "                Val   Loss: 0.6257... Val  Acc: 0.9032... Val  F1: 0.5240...\n",
      "                Test  Loss: 0.5849... Test Acc: 0.9122... Test F1: 0.6228...\n",
      "Epoch: 55/100.. Train Loss: 0.0197\n",
      "                Val   Loss: 0.6475... Val  Acc: 0.9034... Val  F1: 0.5410...\n",
      "                Test  Loss: 0.5873... Test Acc: 0.9156... Test F1: 0.6419...\n",
      "Epoch: 56/100.. Train Loss: 0.0214\n",
      "                Val   Loss: 0.6387... Val  Acc: 0.9001... Val  F1: 0.5820...\n",
      "                Test  Loss: 0.5701... Test Acc: 0.9165... Test F1: 0.6432...\n",
      "Epoch: 57/100.. Train Loss: 0.0204\n",
      "                Val   Loss: 0.7012... Val  Acc: 0.8977... Val  F1: 0.5252...\n",
      "                Test  Loss: 0.6209... Test Acc: 0.9157... Test F1: 0.6542...\n",
      "Epoch: 58/100.. Train Loss: 0.0229\n",
      "                Val   Loss: 0.6663... Val  Acc: 0.8979... Val  F1: 0.5227...\n",
      "                Test  Loss: 0.5812... Test Acc: 0.9165... Test F1: 0.6668...\n",
      "Epoch: 59/100.. Train Loss: 0.0217\n",
      "                Val   Loss: 0.6317... Val  Acc: 0.8966... Val  F1: 0.5164...\n",
      "                Test  Loss: 0.5718... Test Acc: 0.9157... Test F1: 0.6112...\n",
      "Epoch: 60/100.. Train Loss: 0.0241\n",
      "                Val   Loss: 0.5715... Val  Acc: 0.9026... Val  F1: 0.5145...\n",
      "                Test  Loss: 0.5503... Test Acc: 0.9159... Test F1: 0.6466...\n",
      "Epoch: 61/100.. Train Loss: 0.0242\n",
      "                Val   Loss: 0.5654... Val  Acc: 0.8929... Val  F1: 0.4849...\n",
      "                Test  Loss: 0.5528... Test Acc: 0.9069... Test F1: 0.5794...\n",
      "Epoch: 62/100.. Train Loss: 0.0247\n",
      "                Val   Loss: 0.5569... Val  Acc: 0.9011... Val  F1: 0.5408...\n",
      "                Test  Loss: 0.6109... Test Acc: 0.9105... Test F1: 0.6230...\n",
      "Epoch: 63/100.. Train Loss: 0.0229\n",
      "                Val   Loss: 0.5658... Val  Acc: 0.9017... Val  F1: 0.5319...\n",
      "                Test  Loss: 0.6208... Test Acc: 0.9170... Test F1: 0.6676...\n",
      "Epoch: 64/100.. Train Loss: 0.0218\n",
      "                Val   Loss: 0.6048... Val  Acc: 0.9026... Val  F1: 0.6128...\n",
      "                Test  Loss: 0.6206... Test Acc: 0.9142... Test F1: 0.6468...\n",
      "Epoch: 65/100.. Train Loss: 0.0317\n",
      "                Val   Loss: 0.5407... Val  Acc: 0.9042... Val  F1: 0.5489...\n",
      "                Test  Loss: 0.5973... Test Acc: 0.9142... Test F1: 0.6489...\n",
      "Epoch: 66/100.. Train Loss: 0.0177\n",
      "                Val   Loss: 0.5379... Val  Acc: 0.9043... Val  F1: 0.5501...\n",
      "                Test  Loss: 0.6314... Test Acc: 0.9121... Test F1: 0.6118...\n",
      "Epoch: 67/100.. Train Loss: 0.0272\n",
      "                Val   Loss: 0.5650... Val  Acc: 0.9030... Val  F1: 0.5537...\n",
      "                Test  Loss: 0.5932... Test Acc: 0.9141... Test F1: 0.5961...\n",
      "Epoch: 68/100.. Train Loss: 0.0220\n",
      "                Val   Loss: 0.6270... Val  Acc: 0.8981... Val  F1: 0.5182...\n",
      "                Test  Loss: 0.6118... Test Acc: 0.9131... Test F1: 0.6237...\n",
      "Epoch: 69/100.. Train Loss: 0.0206\n",
      "                Val   Loss: 0.6016... Val  Acc: 0.9034... Val  F1: 0.5703...\n",
      "                Test  Loss: 0.6082... Test Acc: 0.9143... Test F1: 0.6046...\n",
      "Epoch: 70/100.. Train Loss: 0.0161\n",
      "                Val   Loss: 0.6055... Val  Acc: 0.9073... Val  F1: 0.5707...\n",
      "                Test  Loss: 0.6109... Test Acc: 0.9200... Test F1: 0.6619...\n",
      "Epoch: 71/100.. Train Loss: 0.0229\n",
      "                Val   Loss: 0.5872... Val  Acc: 0.8999... Val  F1: 0.5473...\n",
      "                Test  Loss: 0.5552... Test Acc: 0.9176... Test F1: 0.6371...\n",
      "Epoch: 72/100.. Train Loss: 0.0180\n",
      "                Val   Loss: 0.6116... Val  Acc: 0.9046... Val  F1: 0.5241...\n",
      "                Test  Loss: 0.6025... Test Acc: 0.9163... Test F1: 0.6142...\n",
      "Epoch: 73/100.. Train Loss: 0.0160\n",
      "                Val   Loss: 0.5920... Val  Acc: 0.9036... Val  F1: 0.5277...\n",
      "                Test  Loss: 0.6283... Test Acc: 0.9126... Test F1: 0.6299...\n",
      "Epoch: 74/100.. Train Loss: 0.0355\n",
      "                Val   Loss: 0.5728... Val  Acc: 0.8963... Val  F1: 0.4892...\n",
      "                Test  Loss: 0.4975... Test Acc: 0.9058... Test F1: 0.5745...\n",
      "Epoch: 75/100.. Train Loss: 0.0283\n",
      "                Val   Loss: 0.5797... Val  Acc: 0.9062... Val  F1: 0.4962...\n",
      "                Test  Loss: 0.5919... Test Acc: 0.9103... Test F1: 0.6321...\n",
      "Epoch: 76/100.. Train Loss: 0.0154\n",
      "                Val   Loss: 0.5781... Val  Acc: 0.9069... Val  F1: 0.5548...\n",
      "                Test  Loss: 0.6355... Test Acc: 0.9086... Test F1: 0.5936...\n",
      "Epoch: 77/100.. Train Loss: 0.0164\n",
      "                Val   Loss: 0.6210... Val  Acc: 0.9039... Val  F1: 0.5484...\n",
      "                Test  Loss: 0.6254... Test Acc: 0.9162... Test F1: 0.6393...\n",
      "Epoch: 78/100.. Train Loss: 0.0159\n",
      "                Val   Loss: 0.5762... Val  Acc: 0.9080... Val  F1: 0.5635...\n",
      "                Test  Loss: 0.6129... Test Acc: 0.9116... Test F1: 0.6081...\n",
      "Epoch: 79/100.. Train Loss: 0.0175\n",
      "                Val   Loss: 0.6399... Val  Acc: 0.9022... Val  F1: 0.5522...\n",
      "                Test  Loss: 0.6394... Test Acc: 0.9159... Test F1: 0.6216...\n",
      "Epoch: 80/100.. Train Loss: 0.0239\n",
      "                Val   Loss: 0.5609... Val  Acc: 0.9036... Val  F1: 0.5464...\n",
      "                Test  Loss: 0.6049... Test Acc: 0.9120... Test F1: 0.6088...\n",
      "Epoch: 81/100.. Train Loss: 0.0220\n",
      "                Val   Loss: 0.6115... Val  Acc: 0.9039... Val  F1: 0.5576...\n",
      "                Test  Loss: 0.6151... Test Acc: 0.9148... Test F1: 0.6293...\n",
      "Epoch: 82/100.. Train Loss: 0.0195\n",
      "                Val   Loss: 0.6689... Val  Acc: 0.8933... Val  F1: 0.5440...\n",
      "                Test  Loss: 0.6707... Test Acc: 0.9145... Test F1: 0.6256...\n",
      "Epoch: 83/100.. Train Loss: 0.0203\n",
      "                Val   Loss: 0.5789... Val  Acc: 0.9002... Val  F1: 0.5479...\n",
      "                Test  Loss: 0.6449... Test Acc: 0.9119... Test F1: 0.6094...\n",
      "Epoch: 84/100.. Train Loss: 0.0233\n",
      "                Val   Loss: 0.6400... Val  Acc: 0.8958... Val  F1: 0.5204...\n",
      "                Test  Loss: 0.5939... Test Acc: 0.9136... Test F1: 0.6155...\n",
      "Epoch: 85/100.. Train Loss: 0.0179\n",
      "                Val   Loss: 0.6095... Val  Acc: 0.8980... Val  F1: 0.5010...\n",
      "                Test  Loss: 0.5634... Test Acc: 0.9153... Test F1: 0.6585...\n",
      "Epoch: 86/100.. Train Loss: 0.0198\n",
      "                Val   Loss: 0.6478... Val  Acc: 0.8936... Val  F1: 0.5330...\n",
      "                Test  Loss: 0.6226... Test Acc: 0.9120... Test F1: 0.6247...\n",
      "Epoch: 87/100.. Train Loss: 0.0161\n",
      "                Val   Loss: 0.6346... Val  Acc: 0.8980... Val  F1: 0.5603...\n",
      "                Test  Loss: 0.6027... Test Acc: 0.9149... Test F1: 0.6144...\n",
      "Epoch: 88/100.. Train Loss: 0.0133\n",
      "                Val   Loss: 0.6813... Val  Acc: 0.8872... Val  F1: 0.5526...\n",
      "                Test  Loss: 0.6362... Test Acc: 0.9122... Test F1: 0.6077...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100.. Train Loss: 0.0179\n",
      "                Val   Loss: 0.6439... Val  Acc: 0.8875... Val  F1: 0.5213...\n",
      "                Test  Loss: 0.5527... Test Acc: 0.9185... Test F1: 0.6265...\n",
      "Epoch: 90/100.. Train Loss: 0.0200\n",
      "                Val   Loss: 0.6183... Val  Acc: 0.9041... Val  F1: 0.5068...\n",
      "                Test  Loss: 0.5752... Test Acc: 0.9145... Test F1: 0.6301...\n",
      "Epoch: 91/100.. Train Loss: 0.0213\n",
      "                Val   Loss: 0.6252... Val  Acc: 0.8908... Val  F1: 0.4876...\n",
      "                Test  Loss: 0.5530... Test Acc: 0.9134... Test F1: 0.6341...\n",
      "Epoch: 92/100.. Train Loss: 0.0167\n",
      "                Val   Loss: 0.6465... Val  Acc: 0.8964... Val  F1: 0.4959...\n",
      "                Test  Loss: 0.5871... Test Acc: 0.9182... Test F1: 0.6637...\n",
      "Epoch: 93/100.. Train Loss: 0.0193\n",
      "                Val   Loss: 0.6026... Val  Acc: 0.9029... Val  F1: 0.5246...\n",
      "                Test  Loss: 0.5631... Test Acc: 0.9176... Test F1: 0.6132...\n",
      "Epoch: 94/100.. Train Loss: 0.0284\n",
      "                Val   Loss: 0.5847... Val  Acc: 0.9079... Val  F1: 0.5283...\n",
      "                Test  Loss: 0.5564... Test Acc: 0.9158... Test F1: 0.6178...\n",
      "Epoch: 95/100.. Train Loss: 0.0178\n",
      "                Val   Loss: 0.6040... Val  Acc: 0.9083... Val  F1: 0.5784...\n",
      "                Test  Loss: 0.5792... Test Acc: 0.9188... Test F1: 0.6318...\n",
      "Epoch: 96/100.. Train Loss: 0.0132\n",
      "                Val   Loss: 0.6475... Val  Acc: 0.9076... Val  F1: 0.5215...\n",
      "                Test  Loss: 0.5989... Test Acc: 0.9192... Test F1: 0.6234...\n",
      "Epoch: 97/100.. Train Loss: 0.0108\n",
      "                Val   Loss: 0.6675... Val  Acc: 0.9060... Val  F1: 0.5164...\n",
      "                Test  Loss: 0.6051... Test Acc: 0.9203... Test F1: 0.6223...\n",
      "Epoch: 98/100.. Train Loss: 0.0115\n",
      "                Val   Loss: 0.6666... Val  Acc: 0.9038... Val  F1: 0.5458...\n",
      "                Test  Loss: 0.5843... Test Acc: 0.9206... Test F1: 0.6365...\n",
      "Epoch: 99/100.. Train Loss: 0.0130\n",
      "                Val   Loss: 0.6844... Val  Acc: 0.9005... Val  F1: 0.5078...\n",
      "                Test  Loss: 0.5875... Test Acc: 0.9197... Test F1: 0.6262...\n",
      "Epoch: 100/100.. Train Loss: 0.0106\n",
      "                Val   Loss: 0.6761... Val  Acc: 0.8983... Val  F1: 0.4985...\n",
      "                Test  Loss: 0.6110... Test Acc: 0.9181... Test F1: 0.6063...\n"
     ]
    }
   ],
   "source": [
    "train(net) # train and save results & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of LSTMs F1-score: 0.7230\n"
     ]
    }
   ],
   "source": [
    "lstmEnsemble(n_bestM=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
