{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# add the 'src' directory as one where we can import modules\n",
    "src_dir = os.path.join(os.getcwd(), os.pardir, 'src')\n",
    "sys.path.append(src_dir)\n",
    "\n",
    "from data.dataset import loadingDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create results folder\n",
    "!mkdir -p ../models/results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gdown in /data/anaconda/envs/py35/lib/python3.5/site-packages (3.8.1)\n",
      "Requirement already satisfied: filelock in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (3.0.4)\n",
      "Requirement already satisfied: requests in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (2.18.4)\n",
      "Requirement already satisfied: tqdm in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (4.28.1)\n",
      "Requirement already satisfied: six in /data/anaconda/envs/py35/lib/python3.5/site-packages (from gdown) (1.11.0)\n",
      "Requirement already satisfied: idna<2.7,>=2.5 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (2.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (2018.8.24)\n",
      "Requirement already satisfied: urllib3<1.23,>=1.21.1 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (1.22)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /data/anaconda/envs/py35/lib/python3.5/site-packages (from requests->gdown) (3.0.4)\n",
      "\u001b[33mYou are using pip version 18.1, however version 19.0.3 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1nkAwjp1TRB-wnOYBvlRJS_srv2c6Spz7\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/opp.mat\n",
      "177MB [00:01, 121MB/s]  \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1KJ04DWE7nt_PB0Zm9ZaN-Wh-ZYgvBOj-\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/pamap2.mat\n",
      "140MB [00:01, 92.5MB/s] \n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=15Q8oV02h2_e94IWJ9rnKLrSCKPCTW5FS\n",
      "To: /data/home/ml/notebooks/ensemblelstm_pytorch/data/processed/skoda.mat\n",
      "114MB [00:02, 54.4MB/s] \n"
     ]
    }
   ],
   "source": [
    "# run below commands to download datasets from google drive using Gdown tool\n",
    "# Alternatively you can manually download datasets from following url and put them in the data folder\n",
    "# https://goo.gl/wgEuhu\n",
    "\n",
    "!pip install gdown\n",
    "!mkdir -p ../data/processed\n",
    "!gdown https://drive.google.com/uc?id=1nkAwjp1TRB-wnOYBvlRJS_srv2c6Spz7 -O ../data/processed/opp.mat\n",
    "!gdown https://drive.google.com/uc?id=1KJ04DWE7nt_PB0Zm9ZaN-Wh-ZYgvBOj- -O ../data/processed/pamap2.mat\n",
    "!gdown https://drive.google.com/uc?id=15Q8oV02h2_e94IWJ9rnKLrSCKPCTW5FS -O ../data/processed/skoda.mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/processed/opp.mat\n",
      "normalising... zero mean, unit variance\n",
      "normalising...X_train, X_valid, X_test... done\n",
      "loading the 79-dim matData successfully . . .\n",
      "\n",
      "Train data shape: inputs(650972, 79), targets (650972,)\n",
      "Valid data shape: inputs(32224, 79), targets (32224,)\n",
      "Test data shape: inputs(118750, 79), targets (118750,)\n"
     ]
    }
   ],
   "source": [
    "#1 is Opportunity , 2 is PAMAP2, 3 is Skoda\n",
    "dataset = 1\n",
    "\n",
    "if dataset == 1:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 79)\n",
    "\tn_classes = 18\n",
    "\tDB = 79\n",
    "if dataset == 2:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 52)\n",
    "\tn_classes = 12\n",
    "\tDB = 52\n",
    "if dataset == 3:\n",
    "\ttrain_x, valid_x, test_x, train_y, valid_y, test_y = loadingDB('../data/processed/', 60)\n",
    "\tn_classes = 11\n",
    "\tDB = 60\n",
    "    \n",
    "print(\"\\nTrain data shape: inputs{0}, targets {1}\".format(train_x.shape, train_y.shape))\n",
    "print(\"Valid data shape: inputs{0}, targets {1}\".format(valid_x.shape, valid_y.shape))\n",
    "print(\"Test data shape: inputs{0}, targets {1}\".format(test_x.shape ,test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape Validation and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid data shape: inputs(1, 32200, 79), targets (1, 32200)\n",
      "Test data shape: inputs(1, 118700, 79), targets (1, 118700)\n"
     ]
    }
   ],
   "source": [
    "DIM = len(train_x[0])\n",
    "TEST_WIN = 5000\n",
    "\n",
    "valid_bt = 1\n",
    "valid_se = len(valid_x)//valid_bt\n",
    "valid_x = valid_x[:valid_se*valid_bt,]\n",
    "valid_y = np.array(valid_y)\n",
    "valid_y = valid_y[:valid_se*valid_bt,]\n",
    "valid_x = np.reshape(valid_x, (valid_bt, -1, DB))\n",
    "valid_y = np.reshape(valid_y, (valid_bt,-1))\n",
    "print(\"Valid data shape: inputs{0}, targets {1}\".format(valid_x.shape, valid_y.shape))\n",
    "\n",
    "test_bt = 1\n",
    "test_se = len(test_x)//test_bt\n",
    "test_x = test_x[:test_se*test_bt,]\n",
    "test_y = np.array(test_y)\n",
    "test_y = test_y[:test_se*test_bt,]\n",
    "test_x = np.reshape(test_x, (test_bt, -1, DB))\n",
    "test_y = np.reshape(test_y, (test_bt,-1))\n",
    "print(\"Test data shape: inputs{0}, targets {1}\".format(test_x.shape ,test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def making_training_set(train_x, train_y, batch_size):\n",
    "    \n",
    "    seqence_len = len(train_x)//batch_size\n",
    "    \n",
    "    # generate random initial position of sampling for each epoch\n",
    "    indices_start = np.random.randint(low=0, high=len(train_x)-seqence_len, size=(batch_size,))\n",
    "    \n",
    "    indices_all_2d = np.zeros((batch_size, seqence_len))\n",
    "    for i in range(batch_size):\n",
    "        indices_all_2d[i,:] = np.arange(indices_start[i],indices_start[i]+seqence_len)\n",
    "    indices_all = np.reshape(indices_all_2d, (-1))\n",
    "\n",
    "    X_train = np.zeros((batch_size, seqence_len, DIM), dtype=np.float32)\n",
    "    y_train = np.zeros((batch_size, seqence_len), dtype=np.uint8) \n",
    "    for i in range(batch_size):\n",
    "        idx_start = indices_start[i]\n",
    "        idx_end = idx_start+seqence_len\n",
    "        X_train[i,:,:] = train_x[idx_start:idx_end, :]\n",
    "        y_train[i,:] = train_y[idx_start:idx_end]\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_channels=DB, n_hidden=256, n_layers=2, \n",
    "                 n_classes=n_classes, drop_prob=0.5):\n",
    "        super(SingleModel, self).__init__()\n",
    "        \n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_classes = n_classes\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_channels = n_channels\n",
    "        \n",
    "        self.lstm  = nn.LSTM(n_channels, n_hidden, n_layers, dropout=self.drop_prob)\n",
    "        self.fc = nn.Linear(n_hidden, n_classes)\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "    def forward(self, x, hidden, batch_size):\n",
    "        \n",
    "        x = x.permute(1, 0, 2)\n",
    "        x, hidden = self.lstm(x, hidden)\n",
    "        x = self.dropout(x)    \n",
    "        x = x.contiguous().view(-1, self.n_hidden)\n",
    "        out = self.fc(x)\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden\n",
    "    \n",
    "net = SingleModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SingleModel(\n",
       "  (lstm): LSTM(79, 256, num_layers=2, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=18, bias=True)\n",
       "  (dropout): Dropout(p=0.5)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    if type(m) == nn.LSTM:\n",
    "        for name, param in m.named_parameters():\n",
    "            if 'weight_ih' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'weight_hh' in name:\n",
    "                torch.nn.init.orthogonal_(param.data)\n",
    "            elif 'bias' in name:\n",
    "                param.data.fill_(0)\n",
    "    elif type(m) == nn.Linear:\n",
    "        torch.nn.init.orthogonal_(m.weight)\n",
    "        m.bias.data.fill_(0)\n",
    "net.apply(init_weights)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(criterion):\n",
    "    \n",
    "    val_accuracy=0\n",
    "    val_f1score=0\n",
    "    val_losses = []\n",
    "    num_val_process = valid_se//TEST_WIN + 1\n",
    "    val_h = net.init_hidden(valid_bt)\n",
    "    net.eval()\n",
    "\n",
    "    for j in range(num_val_process):\n",
    "        start = j*TEST_WIN\n",
    "        end = np.min((valid_se, start+TEST_WIN))\n",
    "        \n",
    "        x = valid_x[:,start:end,:]\n",
    "        y = valid_y[:,start:end]\n",
    "\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        val_h = tuple([each.data for each in val_h])\n",
    "        \n",
    "        output, val_h = net(inputs, val_h, valid_bt)\n",
    "\n",
    "        val_loss = criterion(output, targets.long())\n",
    "        val_losses.append(val_loss.item())\n",
    "        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == targets.view(*top_class.shape).long()\n",
    "        val_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        val_f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='macro')\n",
    "            \n",
    "    test_accuracy=0\n",
    "    test_f1score=0\n",
    "    test_losses = []\n",
    "    num_test_process = test_se//TEST_WIN + 1\n",
    "    test_h = net.init_hidden(test_bt)\n",
    "    \n",
    "    for j in range(num_test_process):\n",
    "        start = j*TEST_WIN\n",
    "        end = np.min((test_se, start+TEST_WIN))\n",
    "        \n",
    "        x = test_x[:,start:end,:]\n",
    "        y = test_y[:,start:end]\n",
    "\n",
    "        inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "        if(train_on_gpu):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        \n",
    "        test_h = tuple([each.data for each in test_h])\n",
    "        \n",
    "        output, test_h = net(inputs, test_h, test_bt)\n",
    "\n",
    "        test_loss = criterion(output, targets.long())\n",
    "        test_losses.append(test_loss.item())\n",
    "        \n",
    "        top_p, top_class = output.topk(1, dim=1)\n",
    "        equals = top_class == targets.view(*top_class.shape).long()\n",
    "        test_accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "        test_f1score += metrics.f1_score(top_class.cpu(), targets.view(*top_class.shape).long().cpu(), average='macro')\n",
    "        \n",
    "    valid_losses_avg = np.mean(val_losses)\n",
    "    valid_f1_avg = val_f1score/num_val_process\n",
    "    print(' '*16 +\"Val   Loss: {:.4f}...\".format(valid_losses_avg),\n",
    "    \"Val  Acc: {:.4f}...\".format(val_accuracy/num_val_process),\n",
    "    \"Val  F1: {:.4f}...\".format(valid_f1_avg))\n",
    "          \n",
    "    test_losses_avg = np.mean(test_losses)\n",
    "    test_f1_avg = test_f1score/num_test_process\n",
    "    print(' '*16 +\"Test  Loss: {:.4f}...\".format(test_losses_avg),\n",
    "    \"Test Acc: {:.4f}...\".format(test_accuracy/num_test_process),\n",
    "    \"Test F1: {:.4f}...\".format(test_f1_avg))\n",
    "    \n",
    "    net.train() # reset to train mode after iterationg through validation data\n",
    "    \n",
    "    return valid_losses_avg, test_losses_avg, valid_f1_avg, test_f1_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, epochs=100, lr=0.001):\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr) \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "     \n",
    "    train_losses = []    \n",
    "    results = np.empty([0, 5], dtype=np.float32)\n",
    "    net.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        train_loss = 0\n",
    "        train_sz = 0\n",
    "        \n",
    "        #generate random batch size for each epoch\n",
    "        batch_size = np.random.randint(low=128, high=256, size=1)[0]\n",
    "        \n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)      \n",
    "        \n",
    "        x_train, y_train = making_training_set(train_x, train_y, batch_size)\n",
    "        train_len = len(train_x)//batch_size # train_x3D shape: [batch_size,train_len,dim]\n",
    "\n",
    "        pos_start = 0\n",
    "        pos_end = 0\n",
    "        while pos_end < train_len:\n",
    "\n",
    "             # generate a random window length in each training process\n",
    "            curr_win_len = np.random.randint(low=16, high=32, size=1)[0]\n",
    "            \n",
    "            pos_start = pos_end\n",
    "            pos_end += curr_win_len\n",
    "\n",
    "            x = x_train[:,pos_start:pos_end,:]\n",
    "            y = y_train[:,pos_start:pos_end]\n",
    "                        \n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "            \n",
    "            # zero accumulated gradients\n",
    "            opt.zero_grad()   \n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h, batch_size)\n",
    "            #output = net(inputs, batch_size)\n",
    "            loss = criterion(output, targets.long())\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            sample_sz = batch_size*curr_win_len\n",
    "            train_loss += loss.item()*sample_sz\n",
    "            train_sz += sample_sz\n",
    "                      \n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            \n",
    "        #saving the models\n",
    "        PATH = '../models/'+str(DB)+'_'+str(epoch)+'.pth'\n",
    "        torch.save(net.state_dict(), PATH)\n",
    "        \n",
    "        train_loss_avg = train_loss/train_sz\n",
    "        print(\"Epoch: {}/{}..\".format(epoch+1, epochs),\n",
    "        \"Train Loss: {:.4f}\".format(train_loss_avg))\n",
    "        \n",
    "        valid_loss, test_loss, valid_f1, test_f1 = validation(criterion)\n",
    "        \n",
    "        #saving the results\n",
    "        epoch_results = np.zeros(5)\n",
    "        \n",
    "        epoch_results[0] = train_loss_avg\n",
    "        epoch_results[1] = valid_loss\n",
    "        epoch_results[2] = test_loss\n",
    "        epoch_results[3] = valid_f1\n",
    "        epoch_results[4] = test_f1\n",
    "        \n",
    "        results = np.float32(np.vstack((results, epoch_results)))\n",
    "        \n",
    "        PATH = '../models/results/'+str(DB)+'.npy'\n",
    "        np.save(PATH, results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  LSTM Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstmEnsemble(n_bestM=20):\n",
    "\n",
    "    PATH = '../models/results/'+str(DB)+'.npy'\n",
    "    results = np.load(PATH)\n",
    "\n",
    "    valid_col = 3 #third column of results is validation f1 \n",
    "    idx_set = np.argsort(results[:,valid_col])[::-1] # sort results based on validation f1\n",
    "\n",
    "    best_models = []\n",
    "    best_models.append(idx_set[:n_bestM]) # store the epoch number of top n models\n",
    "\n",
    "    prob_M = np.zeros((n_bestM, test_y.size, n_classes))\n",
    "    \n",
    "    for i in range(n_bestM):\n",
    "        idx = best_models[0][i]\n",
    "\n",
    "        model = '../models/'+str(DB)+'_'+str(idx)+\".pth\"\n",
    "        net.load_state_dict(torch.load(model))\n",
    "       \n",
    "        if(train_on_gpu):\n",
    "            net.cuda()\n",
    "\n",
    "        num_test_process = test_se//TEST_WIN + 1\n",
    "        test_accuracy=0\n",
    "        test_f1score=0\n",
    "        test_losses = []\n",
    "        test_h = net.init_hidden(test_bt)\n",
    "        prob_2d = np.zeros((test_y.size, n_classes))\n",
    "\n",
    "        net.eval()\n",
    "        for j in range(num_test_process):\n",
    "            start = j*TEST_WIN\n",
    "            end = np.min((test_se, start+TEST_WIN))\n",
    "\n",
    "            x = test_x[:,start:end,:]\n",
    "            y = test_y[:,start:end]\n",
    "\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y.flatten('F'))\n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            test_h = tuple([each.data for each in test_h])\n",
    "            output, test_h = net(inputs, test_h, test_bt)\n",
    "\n",
    "            prob_2d[start*test_bt:end*test_bt,:] = F.softmax(output).cpu().detach().numpy()\n",
    "\n",
    "        prob_M[i,:,:] = prob_2d #store predictions of each of the top n models\n",
    "\n",
    "    prob_avg = np.mean(prob_M[:,:,:], axis=0) #model fusion by calculating the average of probabilities \n",
    "    fused_pred = np.argmax(prob_avg, axis=1)\n",
    "\n",
    "    f1_fused = metrics.f1_score(test_y.flatten(\"F\"), fused_pred, average='macro')\n",
    "\n",
    "    print(\"Ensemble of LSTMs F1-score: {:.4f}\".format(f1_fused))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100.. Train Loss: 0.7529\n",
      "                Val   Loss: 0.6732... Val  Acc: 0.8028... Val  F1: 0.2242...\n",
      "                Test  Loss: 0.4668... Test Acc: 0.8637... Test F1: 0.3522...\n",
      "Epoch: 2/100.. Train Loss: 0.4088\n",
      "                Val   Loss: 0.4614... Val  Acc: 0.8593... Val  F1: 0.3592...\n",
      "                Test  Loss: 0.4220... Test Acc: 0.8702... Test F1: 0.4104...\n",
      "Epoch: 3/100.. Train Loss: 0.3351\n",
      "                Val   Loss: 0.4478... Val  Acc: 0.8626... Val  F1: 0.3781...\n",
      "                Test  Loss: 0.3628... Test Acc: 0.8865... Test F1: 0.5320...\n",
      "Epoch: 4/100.. Train Loss: 0.2858\n",
      "                Val   Loss: 0.4571... Val  Acc: 0.8690... Val  F1: 0.4760...\n",
      "                Test  Loss: 0.3709... Test Acc: 0.8865... Test F1: 0.5302...\n",
      "Epoch: 5/100.. Train Loss: 0.2912\n",
      "                Val   Loss: 0.3862... Val  Acc: 0.8919... Val  F1: 0.4290...\n",
      "                Test  Loss: 0.4079... Test Acc: 0.8818... Test F1: 0.4932...\n",
      "Epoch: 6/100.. Train Loss: 0.2474\n",
      "                Val   Loss: 0.4168... Val  Acc: 0.8864... Val  F1: 0.5097...\n",
      "                Test  Loss: 0.3598... Test Acc: 0.9005... Test F1: 0.5885...\n",
      "Epoch: 7/100.. Train Loss: 0.1793\n",
      "                Val   Loss: 0.3745... Val  Acc: 0.8946... Val  F1: 0.5457...\n",
      "                Test  Loss: 0.3428... Test Acc: 0.9067... Test F1: 0.6438...\n",
      "Epoch: 8/100.. Train Loss: 0.1539\n",
      "                Val   Loss: 0.4657... Val  Acc: 0.8820... Val  F1: 0.5228...\n",
      "                Test  Loss: 0.3417... Test Acc: 0.8987... Test F1: 0.6705...\n",
      "Epoch: 9/100.. Train Loss: 0.1468\n",
      "                Val   Loss: 0.4006... Val  Acc: 0.8959... Val  F1: 0.5397...\n",
      "                Test  Loss: 0.3856... Test Acc: 0.8868... Test F1: 0.6434...\n",
      "Epoch: 10/100.. Train Loss: 0.1246\n",
      "                Val   Loss: 0.3780... Val  Acc: 0.9025... Val  F1: 0.5555...\n",
      "                Test  Loss: 0.3345... Test Acc: 0.9114... Test F1: 0.6789...\n",
      "Epoch: 11/100.. Train Loss: 0.1265\n",
      "                Val   Loss: 0.4242... Val  Acc: 0.8878... Val  F1: 0.4815...\n",
      "                Test  Loss: 0.3855... Test Acc: 0.8815... Test F1: 0.6446...\n",
      "Epoch: 12/100.. Train Loss: 0.1142\n",
      "                Val   Loss: 0.4593... Val  Acc: 0.8964... Val  F1: 0.5252...\n",
      "                Test  Loss: 0.3558... Test Acc: 0.9124... Test F1: 0.7003...\n",
      "Epoch: 13/100.. Train Loss: 0.1148\n",
      "                Val   Loss: 0.3973... Val  Acc: 0.9113... Val  F1: 0.5902...\n",
      "                Test  Loss: 0.4103... Test Acc: 0.9098... Test F1: 0.6709...\n",
      "Epoch: 14/100.. Train Loss: 0.1167\n",
      "                Val   Loss: 0.4041... Val  Acc: 0.9042... Val  F1: 0.5619...\n",
      "                Test  Loss: 0.3744... Test Acc: 0.9015... Test F1: 0.6713...\n",
      "Epoch: 15/100.. Train Loss: 0.1033\n",
      "                Val   Loss: 0.4052... Val  Acc: 0.8975... Val  F1: 0.5361...\n",
      "                Test  Loss: 0.4373... Test Acc: 0.9127... Test F1: 0.6623...\n",
      "Epoch: 16/100.. Train Loss: 0.0864\n",
      "                Val   Loss: 0.4575... Val  Acc: 0.8940... Val  F1: 0.5808...\n",
      "                Test  Loss: 0.4088... Test Acc: 0.9142... Test F1: 0.6978...\n",
      "Epoch: 17/100.. Train Loss: 0.0793\n",
      "                Val   Loss: 0.4469... Val  Acc: 0.9031... Val  F1: 0.5496...\n",
      "                Test  Loss: 0.3871... Test Acc: 0.9120... Test F1: 0.6775...\n",
      "Epoch: 18/100.. Train Loss: 0.0528\n",
      "                Val   Loss: 0.5080... Val  Acc: 0.8931... Val  F1: 0.5277...\n",
      "                Test  Loss: 0.4262... Test Acc: 0.9182... Test F1: 0.7018...\n",
      "Epoch: 19/100.. Train Loss: 0.0625\n",
      "                Val   Loss: 0.5050... Val  Acc: 0.8907... Val  F1: 0.5477...\n",
      "                Test  Loss: 0.4404... Test Acc: 0.9145... Test F1: 0.7007...\n",
      "Epoch: 20/100.. Train Loss: 0.0786\n",
      "                Val   Loss: 0.4944... Val  Acc: 0.8903... Val  F1: 0.5376...\n",
      "                Test  Loss: 0.3915... Test Acc: 0.9095... Test F1: 0.6698...\n",
      "Epoch: 21/100.. Train Loss: 0.0603\n",
      "                Val   Loss: 0.4698... Val  Acc: 0.8952... Val  F1: 0.5228...\n",
      "                Test  Loss: 0.4236... Test Acc: 0.9203... Test F1: 0.7019...\n",
      "Epoch: 22/100.. Train Loss: 0.0590\n",
      "                Val   Loss: 0.5130... Val  Acc: 0.8957... Val  F1: 0.4946...\n",
      "                Test  Loss: 0.4597... Test Acc: 0.9144... Test F1: 0.6577...\n",
      "Epoch: 23/100.. Train Loss: 0.0533\n",
      "                Val   Loss: 0.4643... Val  Acc: 0.9044... Val  F1: 0.5683...\n",
      "                Test  Loss: 0.4129... Test Acc: 0.9167... Test F1: 0.7034...\n",
      "Epoch: 24/100.. Train Loss: 0.0396\n",
      "                Val   Loss: 0.5171... Val  Acc: 0.9020... Val  F1: 0.5248...\n",
      "                Test  Loss: 0.4898... Test Acc: 0.9104... Test F1: 0.6721...\n",
      "Epoch: 25/100.. Train Loss: 0.0635\n",
      "                Val   Loss: 0.4669... Val  Acc: 0.9022... Val  F1: 0.5140...\n",
      "                Test  Loss: 0.4675... Test Acc: 0.9109... Test F1: 0.6832...\n",
      "Epoch: 26/100.. Train Loss: 0.0526\n",
      "                Val   Loss: 0.5029... Val  Acc: 0.9043... Val  F1: 0.5367...\n",
      "                Test  Loss: 0.4592... Test Acc: 0.9147... Test F1: 0.6950...\n",
      "Epoch: 27/100.. Train Loss: 0.0463\n",
      "                Val   Loss: 0.4917... Val  Acc: 0.9115... Val  F1: 0.5743...\n",
      "                Test  Loss: 0.4517... Test Acc: 0.9194... Test F1: 0.6915...\n",
      "Epoch: 28/100.. Train Loss: 0.0622\n",
      "                Val   Loss: 0.4757... Val  Acc: 0.8972... Val  F1: 0.5208...\n",
      "                Test  Loss: 0.4002... Test Acc: 0.9079... Test F1: 0.6811...\n",
      "Epoch: 29/100.. Train Loss: 0.0492\n",
      "                Val   Loss: 0.5075... Val  Acc: 0.8979... Val  F1: 0.5467...\n",
      "                Test  Loss: 0.4443... Test Acc: 0.9204... Test F1: 0.7001...\n",
      "Epoch: 30/100.. Train Loss: 0.0485\n",
      "                Val   Loss: 0.5207... Val  Acc: 0.8935... Val  F1: 0.5155...\n",
      "                Test  Loss: 0.4399... Test Acc: 0.9186... Test F1: 0.7056...\n",
      "Epoch: 31/100.. Train Loss: 0.0527\n",
      "                Val   Loss: 0.5114... Val  Acc: 0.8955... Val  F1: 0.5239...\n",
      "                Test  Loss: 0.4453... Test Acc: 0.9137... Test F1: 0.6762...\n",
      "Epoch: 32/100.. Train Loss: 0.0530\n",
      "                Val   Loss: 0.5266... Val  Acc: 0.8975... Val  F1: 0.5396...\n",
      "                Test  Loss: 0.5130... Test Acc: 0.9140... Test F1: 0.6606...\n",
      "Epoch: 33/100.. Train Loss: 0.0355\n",
      "                Val   Loss: 0.5817... Val  Acc: 0.8902... Val  F1: 0.5556...\n",
      "                Test  Loss: 0.5225... Test Acc: 0.9136... Test F1: 0.6821...\n",
      "Epoch: 34/100.. Train Loss: 0.0295\n",
      "                Val   Loss: 0.5774... Val  Acc: 0.8941... Val  F1: 0.5386...\n",
      "                Test  Loss: 0.5329... Test Acc: 0.9094... Test F1: 0.6821...\n",
      "Epoch: 35/100.. Train Loss: 0.0361\n",
      "                Val   Loss: 0.5394... Val  Acc: 0.8952... Val  F1: 0.5392...\n",
      "                Test  Loss: 0.5760... Test Acc: 0.9069... Test F1: 0.6788...\n",
      "Epoch: 36/100.. Train Loss: 0.0447\n",
      "                Val   Loss: 0.5422... Val  Acc: 0.8908... Val  F1: 0.4919...\n",
      "                Test  Loss: 0.4331... Test Acc: 0.9154... Test F1: 0.6828...\n",
      "Epoch: 37/100.. Train Loss: 0.0378\n",
      "                Val   Loss: 0.5603... Val  Acc: 0.9001... Val  F1: 0.5731...\n",
      "                Test  Loss: 0.4716... Test Acc: 0.9161... Test F1: 0.6957...\n",
      "Epoch: 38/100.. Train Loss: 0.0346\n",
      "                Val   Loss: 0.6365... Val  Acc: 0.8921... Val  F1: 0.5300...\n",
      "                Test  Loss: 0.5184... Test Acc: 0.9124... Test F1: 0.6846...\n",
      "Epoch: 39/100.. Train Loss: 0.0351\n",
      "                Val   Loss: 0.6584... Val  Acc: 0.8928... Val  F1: 0.4985...\n",
      "                Test  Loss: 0.6020... Test Acc: 0.9096... Test F1: 0.6615...\n",
      "Epoch: 40/100.. Train Loss: 0.0336\n",
      "                Val   Loss: 0.6083... Val  Acc: 0.8893... Val  F1: 0.4892...\n",
      "                Test  Loss: 0.5881... Test Acc: 0.9043... Test F1: 0.6485...\n",
      "Epoch: 41/100.. Train Loss: 0.0267\n",
      "                Val   Loss: 0.5756... Val  Acc: 0.8994... Val  F1: 0.5406...\n",
      "                Test  Loss: 0.6022... Test Acc: 0.9068... Test F1: 0.6806...\n",
      "Epoch: 42/100.. Train Loss: 0.0352\n",
      "                Val   Loss: 0.6177... Val  Acc: 0.8977... Val  F1: 0.5259...\n",
      "                Test  Loss: 0.6095... Test Acc: 0.9113... Test F1: 0.6809...\n",
      "Epoch: 43/100.. Train Loss: 0.0283\n",
      "                Val   Loss: 0.6093... Val  Acc: 0.8967... Val  F1: 0.5397...\n",
      "                Test  Loss: 0.6244... Test Acc: 0.9062... Test F1: 0.6635...\n",
      "Epoch: 44/100.. Train Loss: 0.0252\n",
      "                Val   Loss: 0.5987... Val  Acc: 0.8983... Val  F1: 0.5439...\n",
      "                Test  Loss: 0.6280... Test Acc: 0.9130... Test F1: 0.6812...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 45/100.. Train Loss: 0.0241\n",
      "                Val   Loss: 0.5806... Val  Acc: 0.8994... Val  F1: 0.5436...\n",
      "                Test  Loss: 0.6001... Test Acc: 0.9138... Test F1: 0.6940...\n",
      "Epoch: 46/100.. Train Loss: 0.0202\n",
      "                Val   Loss: 0.6588... Val  Acc: 0.8906... Val  F1: 0.5139...\n",
      "                Test  Loss: 0.6355... Test Acc: 0.9140... Test F1: 0.6830...\n",
      "Epoch: 47/100.. Train Loss: 0.0400\n",
      "                Val   Loss: 0.6402... Val  Acc: 0.8820... Val  F1: 0.4858...\n",
      "                Test  Loss: 0.5809... Test Acc: 0.9037... Test F1: 0.6322...\n",
      "Epoch: 48/100.. Train Loss: 0.0397\n",
      "                Val   Loss: 0.5564... Val  Acc: 0.8975... Val  F1: 0.5311...\n",
      "                Test  Loss: 0.5925... Test Acc: 0.9091... Test F1: 0.6737...\n",
      "Epoch: 49/100.. Train Loss: 0.0281\n",
      "                Val   Loss: 0.5990... Val  Acc: 0.8913... Val  F1: 0.5058...\n",
      "                Test  Loss: 0.5834... Test Acc: 0.9091... Test F1: 0.6899...\n",
      "Epoch: 50/100.. Train Loss: 0.0251\n",
      "                Val   Loss: 0.6610... Val  Acc: 0.8848... Val  F1: 0.4661...\n",
      "                Test  Loss: 0.6344... Test Acc: 0.9125... Test F1: 0.6923...\n",
      "Epoch: 51/100.. Train Loss: 0.0348\n",
      "                Val   Loss: 0.6246... Val  Acc: 0.8907... Val  F1: 0.5034...\n",
      "                Test  Loss: 0.6144... Test Acc: 0.9141... Test F1: 0.6825...\n",
      "Epoch: 52/100.. Train Loss: 0.0279\n",
      "                Val   Loss: 0.5759... Val  Acc: 0.8852... Val  F1: 0.5138...\n",
      "                Test  Loss: 0.5498... Test Acc: 0.9156... Test F1: 0.6897...\n",
      "Epoch: 53/100.. Train Loss: 0.0247\n",
      "                Val   Loss: 0.6213... Val  Acc: 0.8856... Val  F1: 0.5114...\n",
      "                Test  Loss: 0.6068... Test Acc: 0.9139... Test F1: 0.6831...\n",
      "Epoch: 54/100.. Train Loss: 0.0230\n",
      "                Val   Loss: 0.6512... Val  Acc: 0.8873... Val  F1: 0.5216...\n",
      "                Test  Loss: 0.6607... Test Acc: 0.9061... Test F1: 0.6891...\n",
      "Epoch: 55/100.. Train Loss: 0.0207\n",
      "                Val   Loss: 0.7234... Val  Acc: 0.8736... Val  F1: 0.4948...\n",
      "                Test  Loss: 0.6659... Test Acc: 0.9094... Test F1: 0.6776...\n",
      "Epoch: 56/100.. Train Loss: 0.0221\n",
      "                Val   Loss: 0.6594... Val  Acc: 0.8772... Val  F1: 0.4772...\n",
      "                Test  Loss: 0.6517... Test Acc: 0.9099... Test F1: 0.6645...\n",
      "Epoch: 57/100.. Train Loss: 0.0250\n",
      "                Val   Loss: 0.6490... Val  Acc: 0.8902... Val  F1: 0.4866...\n",
      "                Test  Loss: 0.6633... Test Acc: 0.9103... Test F1: 0.6607...\n",
      "Epoch: 58/100.. Train Loss: 0.0251\n",
      "                Val   Loss: 0.5935... Val  Acc: 0.8937... Val  F1: 0.5383...\n",
      "                Test  Loss: 0.6691... Test Acc: 0.9088... Test F1: 0.6281...\n",
      "Epoch: 59/100.. Train Loss: 0.0332\n",
      "                Val   Loss: 0.6266... Val  Acc: 0.8877... Val  F1: 0.5341...\n",
      "                Test  Loss: 0.6122... Test Acc: 0.9108... Test F1: 0.6815...\n",
      "Epoch: 60/100.. Train Loss: 0.0353\n",
      "                Val   Loss: 0.5810... Val  Acc: 0.8875... Val  F1: 0.5351...\n",
      "                Test  Loss: 0.5992... Test Acc: 0.9128... Test F1: 0.6744...\n",
      "Epoch: 61/100.. Train Loss: 0.0287\n",
      "                Val   Loss: 0.5956... Val  Acc: 0.8957... Val  F1: 0.5264...\n",
      "                Test  Loss: 0.5798... Test Acc: 0.9148... Test F1: 0.6738...\n",
      "Epoch: 62/100.. Train Loss: 0.0183\n",
      "                Val   Loss: 0.6215... Val  Acc: 0.8966... Val  F1: 0.5454...\n",
      "                Test  Loss: 0.6388... Test Acc: 0.9121... Test F1: 0.6647...\n",
      "Epoch: 63/100.. Train Loss: 0.0161\n",
      "                Val   Loss: 0.6527... Val  Acc: 0.8960... Val  F1: 0.5281...\n",
      "                Test  Loss: 0.7017... Test Acc: 0.9014... Test F1: 0.6734...\n",
      "Epoch: 64/100.. Train Loss: 0.0206\n",
      "                Val   Loss: 0.6783... Val  Acc: 0.8822... Val  F1: 0.5118...\n",
      "                Test  Loss: 0.6128... Test Acc: 0.9077... Test F1: 0.6724...\n",
      "Epoch: 65/100.. Train Loss: 0.0242\n",
      "                Val   Loss: 0.6658... Val  Acc: 0.8888... Val  F1: 0.5129...\n",
      "                Test  Loss: 0.6207... Test Acc: 0.8993... Test F1: 0.6802...\n",
      "Epoch: 66/100.. Train Loss: 0.0186\n",
      "                Val   Loss: 0.6315... Val  Acc: 0.8905... Val  F1: 0.5165...\n",
      "                Test  Loss: 0.6101... Test Acc: 0.9140... Test F1: 0.6846...\n",
      "Epoch: 67/100.. Train Loss: 0.0255\n",
      "                Val   Loss: 0.7054... Val  Acc: 0.8747... Val  F1: 0.4927...\n",
      "                Test  Loss: 0.5742... Test Acc: 0.9160... Test F1: 0.6886...\n",
      "Epoch: 68/100.. Train Loss: 0.0148\n",
      "                Val   Loss: 0.6755... Val  Acc: 0.8830... Val  F1: 0.4944...\n",
      "                Test  Loss: 0.6492... Test Acc: 0.9081... Test F1: 0.6781...\n",
      "Epoch: 69/100.. Train Loss: 0.0283\n",
      "                Val   Loss: 0.6403... Val  Acc: 0.8720... Val  F1: 0.5033...\n",
      "                Test  Loss: 0.5155... Test Acc: 0.9171... Test F1: 0.7058...\n",
      "Epoch: 70/100.. Train Loss: 0.0229\n",
      "                Val   Loss: 0.6133... Val  Acc: 0.8933... Val  F1: 0.5302...\n",
      "                Test  Loss: 0.5645... Test Acc: 0.9192... Test F1: 0.7099...\n",
      "Epoch: 71/100.. Train Loss: 0.0170\n",
      "                Val   Loss: 0.6319... Val  Acc: 0.8920... Val  F1: 0.5269...\n",
      "                Test  Loss: 0.6017... Test Acc: 0.9134... Test F1: 0.7014...\n",
      "Epoch: 72/100.. Train Loss: 0.0211\n",
      "                Val   Loss: 0.6966... Val  Acc: 0.8822... Val  F1: 0.4896...\n",
      "                Test  Loss: 0.5720... Test Acc: 0.9194... Test F1: 0.7032...\n",
      "Epoch: 73/100.. Train Loss: 0.0192\n",
      "                Val   Loss: 0.6682... Val  Acc: 0.8910... Val  F1: 0.5108...\n",
      "                Test  Loss: 0.6589... Test Acc: 0.9138... Test F1: 0.6859...\n",
      "Epoch: 74/100.. Train Loss: 0.0166\n",
      "                Val   Loss: 0.6846... Val  Acc: 0.8916... Val  F1: 0.5069...\n",
      "                Test  Loss: 0.6422... Test Acc: 0.9147... Test F1: 0.6941...\n",
      "Epoch: 75/100.. Train Loss: 0.0149\n",
      "                Val   Loss: 0.7335... Val  Acc: 0.8870... Val  F1: 0.5240...\n",
      "                Test  Loss: 0.6641... Test Acc: 0.9165... Test F1: 0.6958...\n",
      "Epoch: 76/100.. Train Loss: 0.0161\n",
      "                Val   Loss: 0.6913... Val  Acc: 0.8902... Val  F1: 0.5139...\n",
      "                Test  Loss: 0.6958... Test Acc: 0.9151... Test F1: 0.6837...\n",
      "Epoch: 77/100.. Train Loss: 0.0184\n",
      "                Val   Loss: 0.7587... Val  Acc: 0.8825... Val  F1: 0.4905...\n",
      "                Test  Loss: 0.6804... Test Acc: 0.9139... Test F1: 0.6806...\n",
      "Epoch: 78/100.. Train Loss: 0.0207\n",
      "                Val   Loss: 0.6650... Val  Acc: 0.8890... Val  F1: 0.5291...\n",
      "                Test  Loss: 0.6540... Test Acc: 0.9134... Test F1: 0.6762...\n",
      "Epoch: 79/100.. Train Loss: 0.0178\n",
      "                Val   Loss: 0.6807... Val  Acc: 0.8920... Val  F1: 0.5288...\n",
      "                Test  Loss: 0.7214... Test Acc: 0.9128... Test F1: 0.6635...\n",
      "Epoch: 80/100.. Train Loss: 0.0177\n",
      "                Val   Loss: 0.6327... Val  Acc: 0.8930... Val  F1: 0.5431...\n",
      "                Test  Loss: 0.6307... Test Acc: 0.9139... Test F1: 0.6807...\n",
      "Epoch: 81/100.. Train Loss: 0.0197\n",
      "                Val   Loss: 0.7027... Val  Acc: 0.8878... Val  F1: 0.5282...\n",
      "                Test  Loss: 0.6752... Test Acc: 0.9107... Test F1: 0.6944...\n",
      "Epoch: 82/100.. Train Loss: 0.0137\n",
      "                Val   Loss: 0.7092... Val  Acc: 0.8900... Val  F1: 0.5358...\n",
      "                Test  Loss: 0.7251... Test Acc: 0.9102... Test F1: 0.6900...\n",
      "Epoch: 83/100.. Train Loss: 0.0154\n",
      "                Val   Loss: 0.6558... Val  Acc: 0.8902... Val  F1: 0.5578...\n",
      "                Test  Loss: 0.6377... Test Acc: 0.9162... Test F1: 0.7048...\n",
      "Epoch: 84/100.. Train Loss: 0.0190\n",
      "                Val   Loss: 0.7269... Val  Acc: 0.8880... Val  F1: 0.5296...\n",
      "                Test  Loss: 0.6303... Test Acc: 0.9157... Test F1: 0.6859...\n",
      "Epoch: 85/100.. Train Loss: 0.0202\n",
      "                Val   Loss: 0.5642... Val  Acc: 0.8931... Val  F1: 0.5557...\n",
      "                Test  Loss: 0.6068... Test Acc: 0.9098... Test F1: 0.6624...\n",
      "Epoch: 86/100.. Train Loss: 0.0307\n",
      "                Val   Loss: 0.6189... Val  Acc: 0.8931... Val  F1: 0.5508...\n",
      "                Test  Loss: 0.6335... Test Acc: 0.8898... Test F1: 0.6699...\n",
      "Epoch: 87/100.. Train Loss: 0.0254\n",
      "                Val   Loss: 0.5755... Val  Acc: 0.8948... Val  F1: 0.5551...\n",
      "                Test  Loss: 0.5647... Test Acc: 0.9107... Test F1: 0.6796...\n",
      "Epoch: 88/100.. Train Loss: 0.0212\n",
      "                Val   Loss: 0.6270... Val  Acc: 0.8963... Val  F1: 0.5652...\n",
      "                Test  Loss: 0.6717... Test Acc: 0.9056... Test F1: 0.6484...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 89/100.. Train Loss: 0.0153\n",
      "                Val   Loss: 0.7071... Val  Acc: 0.8890... Val  F1: 0.5434...\n",
      "                Test  Loss: 0.7218... Test Acc: 0.9068... Test F1: 0.6526...\n",
      "Epoch: 90/100.. Train Loss: 0.0111\n",
      "                Val   Loss: 0.7257... Val  Acc: 0.8895... Val  F1: 0.5592...\n",
      "                Test  Loss: 0.7419... Test Acc: 0.9077... Test F1: 0.6682...\n",
      "Epoch: 91/100.. Train Loss: 0.0150\n",
      "                Val   Loss: 0.6786... Val  Acc: 0.8943... Val  F1: 0.5399...\n",
      "                Test  Loss: 0.7365... Test Acc: 0.9084... Test F1: 0.6496...\n",
      "Epoch: 92/100.. Train Loss: 0.0123\n",
      "                Val   Loss: 0.6969... Val  Acc: 0.8952... Val  F1: 0.5620...\n",
      "                Test  Loss: 0.7607... Test Acc: 0.9067... Test F1: 0.6539...\n",
      "Epoch: 93/100.. Train Loss: 0.0101\n",
      "                Val   Loss: 0.7272... Val  Acc: 0.8939... Val  F1: 0.5364...\n",
      "                Test  Loss: 0.7924... Test Acc: 0.9030... Test F1: 0.6513...\n",
      "Epoch: 94/100.. Train Loss: 0.0223\n",
      "                Val   Loss: 0.6415... Val  Acc: 0.8914... Val  F1: 0.5504...\n",
      "                Test  Loss: 0.7178... Test Acc: 0.9052... Test F1: 0.6577...\n",
      "Epoch: 95/100.. Train Loss: 0.0202\n",
      "                Val   Loss: 0.5963... Val  Acc: 0.8967... Val  F1: 0.5597...\n",
      "                Test  Loss: 0.6492... Test Acc: 0.9097... Test F1: 0.6550...\n",
      "Epoch: 96/100.. Train Loss: 0.0213\n",
      "                Val   Loss: 0.6950... Val  Acc: 0.8832... Val  F1: 0.5371...\n",
      "                Test  Loss: 0.6551... Test Acc: 0.9098... Test F1: 0.6520...\n",
      "Epoch: 97/100.. Train Loss: 0.0151\n",
      "                Val   Loss: 0.6649... Val  Acc: 0.8912... Val  F1: 0.5260...\n",
      "                Test  Loss: 0.6971... Test Acc: 0.9080... Test F1: 0.6536...\n",
      "Epoch: 98/100.. Train Loss: 0.0154\n",
      "                Val   Loss: 0.7092... Val  Acc: 0.8910... Val  F1: 0.5154...\n",
      "                Test  Loss: 0.7177... Test Acc: 0.9119... Test F1: 0.6755...\n",
      "Epoch: 99/100.. Train Loss: 0.0142\n",
      "                Val   Loss: 0.6598... Val  Acc: 0.8934... Val  F1: 0.5199...\n",
      "                Test  Loss: 0.6599... Test Acc: 0.9130... Test F1: 0.6750...\n",
      "Epoch: 100/100.. Train Loss: 0.0153\n",
      "                Val   Loss: 0.6103... Val  Acc: 0.8901... Val  F1: 0.5690...\n",
      "                Test  Loss: 0.6447... Test Acc: 0.9112... Test F1: 0.6788...\n"
     ]
    }
   ],
   "source": [
    "train(net) # train and save results & models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble of LSTMs F1-score: 0.7179\n"
     ]
    }
   ],
   "source": [
    "lstmEnsemble(n_bestM=20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py35]",
   "language": "python",
   "name": "conda-env-py35-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
